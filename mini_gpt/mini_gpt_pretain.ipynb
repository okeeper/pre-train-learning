{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备环境\n",
    "确保安装依赖：pip install torch jieba matplotlib。\n",
    "准备一个 train.txt 文件，放入足够多的中文文本（建议至少几百个 token）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in ./attention_env/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: appnope in ./attention_env/lib/python3.10/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (8.35.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (1.8.13)\n",
      "Requirement already satisfied: tornado>=6.1 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: psutil in ./attention_env/lib/python3.10/site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: nest-asyncio in ./attention_env/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: pyzmq>=24 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (26.4.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./attention_env/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: packaging in ./attention_env/lib/python3.10/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: exceptiongroup in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
      "Requirement already satisfied: stack_data in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: decorator in ./attention_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./attention_env/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./attention_env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./attention_env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./attention_env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./attention_env/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in ./attention_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./attention_env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./attention_env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./attention_env/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./attention_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: jieba in ./attention_env/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: matplotlib in ./attention_env/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: networkx in ./attention_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in ./attention_env/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: filelock in ./attention_env/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: jinja2 in ./attention_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy in ./attention_env/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./attention_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: pillow>=8 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: numpy>=1.23 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./attention_env/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in ./attention_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./attention_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./attention_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch jieba matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：导入工具库\n",
    "这一步我们加载训练 GPT-2 所需的 Python 库：\n",
    "- **PyTorch**：深度学习框架，提供张量计算和神经网络模块。\n",
    "- **jieba**：中文分词工具，将句子拆成词语。\n",
    "- **matplotlib**：绘图库，用于可视化训练过程中的损失变化。\n",
    "\n",
    "**关键点**：这些库是基础设施，确保我们能处理数据、构建模型并监控训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "工具库导入完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子，确保实验可复现\n",
    "torch.manual_seed(42)\n",
    "print(\"工具库导入完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：定义超参数\n",
    "超参数是模型训练的“设置旋钮”，控制模型的大小和训练行为：\n",
    "- **VOCAB_SIZE**：词汇表大小，决定模型能识别多少个词。\n",
    "- **EMBED_SIZE**：嵌入维度，每个词用多少数字表示其特征。\n",
    "- **NUM_HEADS**：多头注意力的头数，控制注意力机制的并行能力。\n",
    "- **BLOCK_SIZE**：序列长度，一次处理多少个词。\n",
    "- **STRIDE**：滑动窗口步幅，决定数据切分的间隔。\n",
    "- **BATCH_SIZE**：批次大小，一次训练多少个序列。\n",
    "- **NUM_LAYERS**：Transformer 层数，决定模型深度。\n",
    "- **DROPOUT**：丢弃率，防止模型过拟合。\n",
    "- **LEARNING_RATE**：学习率，控制优化步长。\n",
    "- **EPOCHS**：训练轮数，决定训练多久。\n",
    "\n",
    "**原理**：这些参数直接影响模型的容量和学习效率。Transformer 的灵活性在于通过调整这些值，可以适应不同规模的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数设置完成：词汇表=1000，序列长度=32，训练轮数=50\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000       # 词汇表大小\n",
    "EMBED_SIZE = 64         # 嵌入维度\n",
    "NUM_HEADS = 4           # 多头注意力头数\n",
    "BLOCK_SIZE = 32         # 序列长度\n",
    "STRIDE = 4              # 滑动窗口步幅\n",
    "BATCH_SIZE = 8          # 批次大小\n",
    "NUM_LAYERS = 2          # Transformer 层数\n",
    "DROPOUT = 0.1           # Dropout 比例\n",
    "LEARNING_RATE = 0.001   # 学习率\n",
    "EPOCHS = 50             # 训练轮数\n",
    "\n",
    "print(f\"超参数设置完成：词汇表={VOCAB_SIZE}，序列长度={BLOCK_SIZE}，训练轮数={EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三步：构建词汇表\n",
    "词汇表是将文本转换为数字的“字典”：\n",
    "- **分词**：用 jieba 将中文文本拆成词语。\n",
    "- **统计词频**：计算每个词出现的次数，选出最常见的 1000 个。\n",
    "- **编号**：给每个词分配一个唯一索引，未知词用 `<UNK>` 表示。\n",
    "\n",
    "**原理**：语言模型处理的是数字，而非文字。词汇表是桥梁，将人类语言映射到模型能理解的数字空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_file(file_path, max_vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"\n",
    "    从文本文件构建词汇表\n",
    "    :param file_path: 训练文件路径\n",
    "    :param max_vocab_size: 最大词汇表大小\n",
    "    :return: 词汇表字典 {词: 索引}\n",
    "    \"\"\"\n",
    "    word_freq = Counter()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = jieba.cut(line.strip(), cut_all=False)  # 分词\n",
    "            word_freq.update(tokens)\n",
    "    \n",
    "    # 选取高频词，留位置给 <UNK>\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_freq.most_common(max_vocab_size - 1))}\n",
    "    vocab['<UNK>'] = 0\n",
    "    print(f\"词汇表大小: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "def tokens_to_indices(tokens, vocab):\n",
    "    \"\"\"\n",
    "    将词序列转换为索引序列\n",
    "    :param tokens: 分词后的词列表\n",
    "    :param vocab: 词汇表\n",
    "    :return: 索引列表\n",
    "    \"\"\"\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四步：准备训练数据（数据集）\n",
    "数据集是将文本切成小块的过程：\n",
    "- **滑动窗口**：从文本中以固定长度（BLOCK_SIZE）切出序列，步幅（STRIDE）决定每次移动多少。\n",
    "- **自回归任务**：每个序列分为输入和目标，目标是输入的“下一个词”。例如，输入 `[1, 2, 3]`，目标 `[2, 3, 4]`。\n",
    "- **重合率**：步幅越小，序列间重合越多，数据利用率越高。\n",
    "\n",
    "**原理**：GPT-2 通过自回归任务（预测下一个词）学习语言模式。滑动窗口确保模型能看到足够的上下文，同时控制数据量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, block_size, stride, vocab):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param file_path: 训练文件路径\n",
    "        :param block_size: 序列长度\n",
    "        :param stride: 滑动窗口步幅\n",
    "        :param vocab: 词汇表\n",
    "        \"\"\"\n",
    "        self.block_size = block_size\n",
    "        self.stride = stride\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        \n",
    "        # 读取并分词\n",
    "        all_tokens = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = list(jieba.cut(line.strip(), cut_all=False))\n",
    "                all_tokens.extend(tokens)\n",
    "        \n",
    "        # 转换为索引序列\n",
    "        indices = tokens_to_indices(all_tokens, vocab)\n",
    "        \n",
    "        # 滑动窗口切分\n",
    "        for i in range(0, len(indices) - block_size, stride):\n",
    "            seq = indices[i:i + block_size + 1]\n",
    "            self.data.append(seq)\n",
    "        \n",
    "        print(f\"总序列数: {len(self.data)}，重合率: {(block_size - stride) / block_size:.2%}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        return (torch.tensor(seq[:-1], dtype=torch.long),  # 输入序列\n",
    "                torch.tensor(seq[1:], dtype=torch.long))   # 目标序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第五步：添加位置编码\n",
    "Transformer 不像传统模型（如 RNN）能天然记住词的顺序，它需要显式地加入位置信息：\n",
    "- **位置编码**：用数学公式（正弦和余弦函数）为每个词生成一个独特的“位置标记”。\n",
    "- **为什么需要**：没有位置编码，Transformer 会把句子当成一堆无序的词，无法理解“人工智能”和“技术”的前后关系。\n",
    "\n",
    "**设计精髓**：位置编码是固定的，巧妙利用三角函数的周期性，确保不同位置的标记既有区别又有规律，完美适配注意力机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, block_size):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(block_size, embed_size)\n",
    "        for pos in range(block_size):\n",
    "            for i in range(0, embed_size, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / embed_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / embed_size)))\n",
    "        self.register_buffer('pe', pe)  # 固定参数，不参与训练\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1), :]  # 将位置编码加到输入上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第六步：创建嵌入层\n",
    "嵌入层将词索引转换为有意义的数字表示：\n",
    "- **词嵌入**：每个词被映射到一个高维向量（EMBED_SIZE），表示其语义特征。\n",
    "- **位置编码**：在词嵌入上叠加位置信息，确保模型知道词的顺序。\n",
    "\n",
    "**原理**：嵌入层是 Transformer 的输入端，将离散的词转化为连续的向量空间，为后续的注意力计算铺路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_size)  # 词嵌入层\n",
    "        self.pos_enc = PositionalEncoding(embed_size, block_size)  # 位置编码\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)  # 词转为向量\n",
    "        return self.pos_enc(x)  # 加上位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第七步：构建 Transformer Block\n",
    "Transformer Block 是模型的核心计算单元：\n",
    "- **多头自注意力**：像多个侦探同时分析句子，从不同角度找出词之间的关系（例如“人工智能”和“技术”相关）。\n",
    "- **因果掩码**：屏蔽未来的词，确保模型只看当前和之前的词，符合自回归任务（预测下一个词）。\n",
    "- **前馈网络**：对每个词的表示做进一步加工，增强模型的表达能力。\n",
    "- **层归一化**：稳定训练过程，避免数值过大或过小。\n",
    "\n",
    "**设计精髓**：多头注意力是 Transformer 的“超级大脑”，能并行捕捉多种依赖关系；因果掩码保证了时间顺序的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout)  # 多头自注意力\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # 层归一化\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),  # 扩展维度\n",
    "            nn.ReLU(),                              # 激活函数\n",
    "            nn.Linear(4 * embed_size, embed_size),  # 压缩回去\n",
    "            nn.Dropout(dropout),                    # 随机丢弃，防过拟合\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(block_size, block_size), diagonal=1).bool())  # 因果掩码\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        attn_output, _ = self.attn(x, x, x, attn_mask=self.mask[:seq_len, :seq_len])  # 自注意力计算\n",
    "        x = self.norm1(x + attn_output)  # 残差连接 + 归一化\n",
    "        mlp_output = self.mlp(x)         # 前馈网络\n",
    "        x = self.norm2(x + mlp_output)   # 再次残差连接 + 归一化\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第八步：组装 MiniGPT\n",
    "MiniGPT 是简化的 GPT-2 模型：\n",
    "- **嵌入层**：将词索引转为向量表示。\n",
    "- **Transformer Blocks**：多个 Block 叠加，逐层提炼语言特征。\n",
    "- **输出头**：将最终表示映射回词汇表，预测下一个词的概率。\n",
    "\n",
    "**原理**：多层 Transformer Block 的叠加增强了模型的深度，能捕捉更复杂的语言模式；残差连接和归一化确保训练稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, block_size, num_heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingLayer(vocab_size, embed_size, block_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, block_size, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.head = nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)          # 输入转为嵌入向量\n",
    "        x = x.transpose(0, 1)          # 调整为 (seq_len, batch_size, embed_size)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)               # 逐层处理\n",
    "        x = self.ln_f(x)               # 最后归一化\n",
    "        x = x.transpose(0, 1)          # 恢复为 (batch_size, seq_len, embed_size)\n",
    "        logits = self.head(x)          # 输出词的概率分布\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第九步：初始化模型和优化器\n",
    "- **设备**：优先使用 Mac M1 的 MPS 加速计算。\n",
    "- **损失函数**：交叉熵损失，衡量预测词和真实词的差距。\n",
    "- **优化器**：Adam 算法，根据损失调整模型参数。\n",
    "\n",
    "**原理**：优化器是模型的“导航仪”，通过梯度下降逐步优化参数，让预测更准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型初始化完成，运行在 mps 上\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = MiniGPT(VOCAB_SIZE, EMBED_SIZE, BLOCK_SIZE, NUM_HEADS, NUM_LAYERS, DROPOUT).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"模型初始化完成，运行在 {device} 上\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第十步：实现文本生成\n",
    "生成函数测试模型的语言能力：\n",
    "- 输入起始词，模型逐个预测后续词。\n",
    "- 每次选择概率最高的词（贪婪解码），拼接成句子。\n",
    "\n",
    "**原理**：生成过程展示了 Transformer 的自回归特性，体现了它对语言序列的理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, vocab, start_text, max_length=10):\n",
    "    \"\"\"\n",
    "    生成文本\n",
    "    :param model: 训练好的模型\n",
    "    :param vocab: 词汇表\n",
    "    :param start_text: 起始文本\n",
    "    :param max_length: 生成的最大长度\n",
    "    :return: 生成的文本\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    with torch.no_grad():\n",
    "        start_tokens = list(jieba.cut(start_text, cut_all=False))\n",
    "        indices = torch.tensor(tokens_to_indices(start_tokens, vocab), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        for _ in range(max_length):\n",
    "            logits = model(indices)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "            indices = torch.cat([indices, next_token.unsqueeze(0)], dim=1)\n",
    "        return \"\".join([reverse_vocab.get(idx.item(), '<UNK>') for idx in indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第十一步：训练模型\n",
    "训练是模型学习的过程：\n",
    "- **前向传播**：输入序列，计算预测结果。\n",
    "- **损失计算**：比较预测和真实目标，得出误差。\n",
    "- **反向传播**：根据误差调整模型参数。\n",
    "- **监控指标**：记录损失和困惑度（exp(loss)），评估模型性能。\n",
    "- **生成测试**：每轮生成文本，观察学习效果。\n",
    "\n",
    "**设计精髓**：Transformer 通过自注意力机制高效并行处理序列，反向传播让它逐步掌握语言规律。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param model: MiniGPT 模型\n",
    "    :param train_loader: 训练数据加载器\n",
    "    :param epochs: 训练轮数\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_count = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()          # 清空梯度\n",
    "            logits = model(inputs)         # 前向传播\n",
    "            loss = criterion(logits.view(-1, VOCAB_SIZE), targets.view(-1))  # 计算损失\n",
    "            loss.backward()                # 反向传播\n",
    "            optimizer.step()               # 更新参数\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_count += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_perplexity = math.exp(avg_train_loss)  # 困惑度\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.2f}\")\n",
    "        print(f\"  Generated: {generate(model, vocab, '人工智能是', max_length=10)}\")\n",
    "    \n",
    "    # 可视化损失曲线\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第十二步：启动训练\n",
    "- **加载数据**：从文件创建词汇表和数据集。\n",
    "- **训练**：用 DataLoader 分批输入数据，开始学习。\n",
    "- **保存和测试**：保存模型权重，生成最终文本。\n",
    "\n",
    "**原理**：训练是 Transformer 从“零基础”到“语言专家”的过程，DataLoader 的随机打乱增强了泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"train.txt\"  # 训练文件路径\n",
    "\n",
    "print(\"构建词汇表...\")\n",
    "vocab = build_vocab_from_file(file_path)\n",
    "dataset = TextDataset(file_path, BLOCK_SIZE, STRIDE, vocab)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"训练数据量: {len(dataset)} 个序列\")\n",
    "print(\"开始训练...\")\n",
    "\n",
    "train(model, train_loader, EPOCHS)\n",
    "\n",
    "torch.save(model.state_dict(), \"mini_gpt.pth\")\n",
    "print(\"训练完成！模型保存为 'mini_gpt.pth'\")\n",
    "\n",
    "print(\"最终生成文本:\", generate(model, vocab, \"人工智能是\", max_length=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
