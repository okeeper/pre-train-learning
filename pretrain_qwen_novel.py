import os
import json
import glob
from typing import List, Dict, Any, Optional

import torch
from torch.utils.data import Dataset, DataLoader
import transformers
from transformers import (
    AutoModelForCausalLM,  # æ›¿æ¢ QWenModel
    AutoConfig,            # æ›¿æ¢ QWenConfig
    AutoTokenizer,         # æ›¿æ¢ QWenTokenizer
    Trainer, 
    TrainingArguments,
    DataCollatorForLanguageModeling,
    set_seed
)
from transformers.trainer_utils import get_last_checkpoint
import logging
import argparse
from datasets import load_dataset
import wandb  # å¯¼å…¥wandbåŒ…

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

# è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_args():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å°è¯´æ•°æ®é¢„è®­ç»ƒQwenæ¨¡å‹")
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        default="Qwen/Qwen2.5-1.5B-Instruct",
        help="è¦é¢„è®­ç»ƒçš„Qwenæ¨¡å‹è·¯å¾„æˆ–åç§°",
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="data",
        help="åŒ…å«xd_chunks_*.jsonæ–‡ä»¶çš„æ•°æ®ç›®å½•",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="output",
        help="ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—çš„è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=1,
        help="æ¯ä¸ªGPUçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help="åˆå§‹å­¦ä¹ ç‡",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=0.01,
        help="æƒé‡è¡°å‡",
    )
    parser.add_argument(
        "--num_train_epochs",
        type=float,
        default=3.0,
        help="è®­ç»ƒè½®æ¬¡æ•°",
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=-1,  # é»˜è®¤å€¼ä¸º-1ï¼Œè¡¨ç¤ºä½¿ç”¨num_train_epochs
        help="è®­ç»ƒçš„æœ€å¤§æ­¥æ•°ï¼Œè®¾ä¸º-1æ—¶ä½¿ç”¨num_train_epochså‚æ•°",
    )
    parser.add_argument(
        "--logging_steps",
        type=int,
        default=10,
        help="æ—¥å¿—è®°å½•æ­¥æ•°",
    )
    parser.add_argument(
        "--save_steps",
        type=int,
        default=500,
        help="ä¿å­˜æ¨¡å‹çš„æ­¥æ•°",
    )
    parser.add_argument(
        "--max_seq_length",
        type=int,
        default=2048,
        help="æœ€å¤§åºåˆ—é•¿åº¦",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­",
    )
    parser.add_argument(
        "--fp16",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
    )
    parser.add_argument(
        "--deepspeed",
        type=str,
        default=None,
        help="DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--file_pattern",
        type=str,
        default="xd_chunks_*.json",
        help="ç”¨äºåŒ¹é…æ•°æ®æ–‡ä»¶çš„é€šé…ç¬¦æ¨¡å¼ï¼Œå¯ç”¨é€—å·åˆ†éš”å¤šä¸ªæ¨¡å¼",
    )
    parser.add_argument(
        "--local_rank",
        type=int,
        default=-1,
        help="ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒçš„æœ¬åœ°æ’å",
    )
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true", 
        default=True,
        help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜",
    )
    parser.add_argument(
        "--use_wandb",
        action="store_true",
        default=True,
        help="æ˜¯å¦ä½¿ç”¨Weights & Biasesè¿›è¡Œå®éªŒè·Ÿè¸ª",
    )
    parser.add_argument(
        "--wandb_project",
        type=str,
        help="Weights & Biasesé¡¹ç›®åç§°",
    )
    parser.add_argument(
        "--wandb_name",
        type=str,
        default=None,
        help="Weights & Biasesè¿è¡Œåç§°",
    )
    parser.add_argument(
        "--wandb_watch",
        type=str,
        default="gradients",
        choices=["all", "gradients", "parameters", "False"],
        help="wandbçš„watchçº§åˆ« (default: gradients)",
    )
    args = parser.parse_args()
    return args

# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½å¤šä¸ªJSONæ–‡ä»¶
class NovelChunksDataset(Dataset):
    def __init__(self, data_dir: str, tokenizer: AutoTokenizer, max_seq_length: int, file_pattern: str = "xd_chunks_*.json"):
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.examples = []
        
        # å¤„ç†é€—å·åˆ†éš”çš„æ¨¡å¼
        patterns = [p.strip() for p in file_pattern.split(',')]
        json_files = []
        
        # è·å–æ‰€æœ‰ç¬¦åˆæ–‡ä»¶æ¨¡å¼çš„JSONæ–‡ä»¶
        for pattern in patterns:
            matched_files = glob.glob(os.path.join(data_dir, pattern))
            json_files.extend(matched_files)
        
        # å»é™¤å¯èƒ½çš„é‡å¤æ–‡ä»¶
        json_files = list(set(json_files))
        
        logger.info(f"æ‰¾åˆ°{len(json_files)}ä¸ªæ•°æ®æ–‡ä»¶: {json_files}")
        
        # åŠ è½½æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and "content" in item:
                                self.examples.append(item["content"])
                            elif isinstance(item, str):
                                self.examples.append(item)
                    elif isinstance(data, dict) and "content" in data:
                        self.examples.append(data["content"])
                    else:
                        logger.warning(f"æ–‡ä»¶{json_file}æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: {type(data)}")
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶{json_file}æ—¶å‡ºé”™: {str(e)}")
        
        logger.info(f"æ€»å…±åŠ è½½äº†{len(self.examples)}ä¸ªæ–‡æœ¬ç‰‡æ®µ")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        text = self.examples[idx]
        
        # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
        encodings = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_seq_length,
            padding="max_length",
            return_tensors="pt",
        )
        
        # éœ€è¦å°†å¼ é‡ä»å½¢çŠ¶[1, seq_len]è½¬æ¢ä¸º[seq_len]
        item = {
            "input_ids": encodings["input_ids"].squeeze(0),
            "attention_mask": encodings["attention_mask"].squeeze(0),
        }
        
        # ä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒè®¾ç½®æ ‡ç­¾
        item["labels"] = item["input_ids"].clone()
        
        return item

def setup_parallel_training(args):
    """é…ç½®å¹¶è¡Œè®­ç»ƒç¯å¢ƒ"""
    num_gpus = torch.cuda.device_count()
    logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPUè®¾å¤‡")
    
    if num_gpus > 1:
        logger.info(f"å°†ä½¿ç”¨æ‰€æœ‰ {num_gpus} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        # å¦‚æœæœªæŒ‡å®šdeepspeedä½†æœ‰å¤šä¸ªGPUå¯ç”¨ï¼Œè‡ªåŠ¨ç”Ÿæˆé…ç½®
        if args.deepspeed is None:
            logger.info("è‡ªåŠ¨ç”ŸæˆDeepSpeedé…ç½®")
            ds_config = {
                "fp16": {"enabled": args.fp16},
                "optimizer": {
                    "type": "AdamW",
                    "params": {
                        "lr": args.learning_rate,
                        "weight_decay": args.weight_decay
                    }
                },
                "zero_optimization": {
                    "stage": 2,
                    "offload_optimizer": {"device": "cpu", "pin_memory": True},
                    "contiguous_gradients": True,
                    "overlap_comm": True
                },
                "gradient_accumulation_steps": args.gradient_accumulation_steps,
                "train_batch_size": args.per_device_train_batch_size * num_gpus * args.gradient_accumulation_steps,
                "train_micro_batch_size_per_gpu": args.per_device_train_batch_size
            }
            
            # ä¿å­˜ç”Ÿæˆçš„é…ç½®
            ds_config_path = os.path.join(args.output_dir, "ds_auto_config.json")
            os.makedirs(args.output_dir, exist_ok=True)
            with open(ds_config_path, 'w') as f:
                json.dump(ds_config, f, indent=2)
            
            args.deepspeed = ds_config_path
            logger.info(f"è‡ªåŠ¨ç”Ÿæˆçš„DeepSpeedé…ç½®å·²ä¿å­˜åˆ°ï¼š{ds_config_path}")
    
    return args

def setup_wandb(args):
    """è®¾ç½®Weights & Biasesè®°å½•"""
    if args.use_wandb:
        logger.info("åˆå§‹åŒ–Weights & Biases")
        run_name = args.wandb_name if args.wandb_name else f"qwen-pretrain-{args.model_name_or_path.split('/')[-1]}"
        
        # åˆå§‹åŒ–wandb
        wandb.init(
            project=args.wandb_project,
            name=run_name,
            config=vars(args)
        )
        
        # è®°å½•ç¯å¢ƒä¿¡æ¯
        wandb.config.update({
            "gpu_count": torch.cuda.device_count(),
            "gpu_type": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None",
            "pytorch_version": torch.__version__,
            "transformers_version": transformers.__version__,
        })
        
        logger.info(f"Weights & Biaseså·²åˆå§‹åŒ–: {wandb.run.name}")
        return True
    return False

def print_training_config(args, model_config, train_dataset, effective_batch_size):
    """æ‰“å°è®­ç»ƒé…ç½®çš„æ¼‚äº®æ ¼å¼"""
    import time
    from datetime import datetime
    
    # ä½¿ç”¨Unicodeå­—ç¬¦åˆ›å»ºæ›´ç²¾ç¾çš„æ¡†æ¶
    top_border = "â•”" + "â•" * 78 + "â•—"
    bottom_border = "â•š" + "â•" * 78 + "â•"
    side_border = "â•‘"
    mid_border = "â• " + "â•" * 78 + "â•£"
    
    # è·å–å½“å‰æ—¶é—´
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    gpu_count = torch.cuda.device_count()
    
    # ä¼°ç®—æ¨¡å‹å‚æ•°é‡
    params_count = ""
    if hasattr(model_config, "num_hidden_layers") and hasattr(model_config, "hidden_size"):
        # ç²—ç•¥ä¼°è®¡å‚æ•°é‡
        hidden_size = model_config.hidden_size
        n_layers = model_config.num_hidden_layers
        if hasattr(model_config, "vocab_size"):
            vocab_size = model_config.vocab_size
            estimated_params = (12 * hidden_size * hidden_size * n_layers) / 1_000_000
            params_count = f"çº¦ {estimated_params:.1f}B å‚æ•°"
    
    # æ‰“å°ç²¾ç¾çš„é…ç½®ä¿¡æ¯
    print("\n\n")
    print(top_border)
    print(f"{side_border}{'Qwen é¢„è®­ç»ƒé…ç½®':^78}{side_border}")
    print(mid_border)
    
    # åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†
    print(f"{side_border} {'ğŸ“… å¼€å§‹æ—¶é—´:':<25} {now:<50} {side_border}")
    print(f"{side_border} {'ğŸ§  æ¨¡å‹åç§°:':<25} {args.model_name_or_path:<50} {side_border}")
    if params_count:
        print(f"{side_border} {'ğŸ“Š æ¨¡å‹è§„æ¨¡:':<25} {params_count:<50} {side_border}")
    
    # æ¨¡å‹æ¶æ„éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸ—ï¸ æ¨¡å‹æ¶æ„':^78} {side_border}")
    print(f"{side_border}    {'- éšè—å±‚æ•°:':<23} {model_config.num_hidden_layers:<48} {side_border}")
    print(f"{side_border}    {'- éšè—ç»´åº¦:':<23} {model_config.hidden_size:<48} {side_border}")
    if hasattr(model_config, "num_attention_heads"):
        print(f"{side_border}    {'- æ³¨æ„åŠ›å¤´æ•°:':<23} {model_config.num_attention_heads:<48} {side_border}")
    if hasattr(model_config, "vocab_size"):
        print(f"{side_border}    {'- è¯è¡¨å¤§å°:':<23} {model_config.vocab_size:<48} {side_border}")
    
    # è®­ç»ƒæ•°æ®éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸ“š è®­ç»ƒæ•°æ®':^78} {side_border}")
    print(f"{side_border}    {'- æ•°æ®ç›®å½•:':<23} {args.data_dir:<48} {side_border}")
    print(f"{side_border}    {'- æ–‡ä»¶æ¨¡å¼:':<23} {args.file_pattern:<48} {side_border}")
    print(f"{side_border}    {'- æ•°æ®æ ·æœ¬æ•°:':<23} {len(train_dataset):,} ä¸ªæ ·æœ¬{' '*(48-len(str(len(train_dataset)))-5)} {side_border}")
    print(f"{side_border}    {'- æœ€å¤§åºåˆ—é•¿åº¦:':<23} {args.max_seq_length}{' '*(48-len(str(args.max_seq_length)))} {side_border}")
    
    # è®­ç»ƒè®¾ç½®éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'âš™ï¸ è®­ç»ƒè®¾ç½®':^78} {side_border}")
    print(f"{side_border}    {'- GPUæ•°é‡:':<23} {gpu_count} {'GPU':<47} {side_border}")
    print(f"{side_border}    {'- æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°:':<23} {args.per_device_train_batch_size}{' '*(48-len(str(args.per_device_train_batch_size)))} {side_border}")
    print(f"{side_border}    {'- æ¢¯åº¦ç´¯ç§¯æ­¥æ•°:':<23} {args.gradient_accumulation_steps}{' '*(48-len(str(args.gradient_accumulation_steps)))} {side_border}")
    print(f"{side_border}    {'- æœ‰æ•ˆæ€»æ‰¹æ¬¡å¤§å°:':<23} {effective_batch_size}{' '*(48-len(str(effective_batch_size)))} {side_border}")
    
    if args.max_steps > 0:
        print(f"{side_border}    {'- è®­ç»ƒæ­¥æ•°:':<23} {args.max_steps:,}{' '*(48-len(str(args.max_steps))-2)} {side_border}")
        total_samples = args.max_steps * effective_batch_size
        epochs_equiv = args.max_steps * effective_batch_size / len(train_dataset)
        print(f"{side_border}    {'- é¢„è®¡è®­ç»ƒæ ·æœ¬æ•°:':<23} {total_samples:,} (çº¦ {epochs_equiv:.2f} è½®){' '*(48-len(str(total_samples))-len(f'(çº¦ {epochs_equiv:.2f} è½®)')-2)} {side_border}")
    else:
        print(f"{side_border}    {'- è®­ç»ƒè½®æ¬¡:':<23} {args.num_train_epochs:.1f} è½®{' '*(48-len(str(args.num_train_epochs))-3)} {side_border}")
        estimated_steps = int(len(train_dataset) * args.num_train_epochs / effective_batch_size)
        print(f"{side_border}    {'- é¢„è®¡æ€»æ­¥æ•°:':<23} {estimated_steps:,}{' '*(48-len(str(estimated_steps))-2)} {side_border}")
    
    # ä¼˜åŒ–å™¨è®¾ç½®éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸ”§ ä¼˜åŒ–å™¨è®¾ç½®':^78} {side_border}")
    print(f"{side_border}    {'- å­¦ä¹ ç‡:':<23} {args.learning_rate:.1e}{' '*(48-len(f'{args.learning_rate:.1e}'))} {side_border}")
    print(f"{side_border}    {'- æƒé‡è¡°å‡:':<23} {args.weight_decay}{' '*(48-len(str(args.weight_decay)))} {side_border}")
    
    # åŠ é€ŸæŠ€æœ¯éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸš€ åŠ é€ŸæŠ€æœ¯':^78} {side_border}")
    print(f"{side_border}    {'- FP16æ··åˆç²¾åº¦:':<23} {'âœ… å¯ç”¨' if args.fp16 else 'âŒ ç¦ç”¨':<48} {side_border}")
    print(f"{side_border}    {'- æ¢¯åº¦æ£€æŸ¥ç‚¹:':<23} {'âœ… å¯ç”¨' if args.gradient_checkpointing else 'âŒ ç¦ç”¨':<48} {side_border}")
    print(f"{side_border}    {'- DeepSpeed:':<23} {'âœ… å¯ç”¨' if args.deepspeed else 'âŒ ç¦ç”¨':<48} {side_border}")
    
    # ä¿å­˜ä¸ç›‘æ§éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸ“Š ä¿å­˜ä¸ç›‘æ§':^78} {side_border}")
    print(f"{side_border}    {'- è¾“å‡ºç›®å½•:':<23} {args.output_dir:<48} {side_border}")
    print(f"{side_border}    {'- æ—¥å¿—æ­¥æ•°:':<23} {args.logging_steps}{' '*(48-len(str(args.logging_steps)))} {side_border}")
    print(f"{side_border}    {'- ä¿å­˜æ­¥æ•°:':<23} {args.save_steps}{' '*(48-len(str(args.save_steps)))} {side_border}")
    print(f"{side_border}    {'- Weights & Biases:':<23} {'âœ… å¯ç”¨' if args.use_wandb else 'âŒ ç¦ç”¨':<48} {side_border}")
    if args.use_wandb and wandb.run:
        print(f"{side_border}    {'- WandBé¡¹ç›®:':<23} {args.wandb_project:<48} {side_border}")
        print(f"{side_border}    {'- WandBè¿è¡Œ:':<23} {wandb.run.name:<48} {side_border}")
    
    # å…¶ä»–ä¿¡æ¯éƒ¨åˆ†
    print(mid_border)
    print(f"{side_border} {'ğŸ”„ å…¶ä»–ä¿¡æ¯':^78} {side_border}")
    print(f"{side_border}    {'- éšæœºç§å­:':<23} {args.seed}{' '*(48-len(str(args.seed)))} {side_border}")
    
    # é¢„è®¡çš„è®­ç»ƒæ—¶é—´
    # è¿™é‡Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œç²—ç•¥ä¼°è®¡
    tokens_per_step = effective_batch_size * args.max_seq_length
    if args.max_steps > 0:
        total_steps = args.max_steps
    else:
        total_steps = int(len(train_dataset) * args.num_train_epochs / effective_batch_size)
    
    # è®¡ç®—ç²—ç•¥çš„è®­ç»ƒæ—¶é—´ä¼°è®¡ï¼ˆå‡è®¾æ¯æ­¥è®­ç»ƒæ—¶é—´ï¼‰
    tokens_per_second = 1000  # ç²—ç•¥ä¼°è®¡ï¼Œå®é™…å–å†³äºç¡¬ä»¶
    if gpu_count > 0 and hasattr(model_config, 'hidden_size'):
        # æ ¹æ®æ¨¡å‹å¤§å°å’ŒGPUæ•°é‡ç²—ç•¥ä¼°è®¡
        size_factor = model_config.hidden_size / 1024
        tokens_per_second = tokens_per_second / size_factor * gpu_count
        
    estimated_seconds = (tokens_per_step * total_steps) / tokens_per_second
    estimated_hours = estimated_seconds / 3600
    
    days = int(estimated_hours // 24)
    hours = int(estimated_hours % 24)
    minutes = int((estimated_hours * 60) % 60)
    
    time_str = ""
    if days > 0:
        time_str += f"{days}å¤© "
    time_str += f"{hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
    
    print(f"{side_border}    {'- é¢„è®¡è®­ç»ƒæ—¶é•¿:':<23} {time_str:<48} {side_border}")
    
    # åº•éƒ¨è¾¹æ¡†å’Œæç¤º
    print(mid_border)
    print(f"{side_border} {'â³ è®­ç»ƒå·²å¼€å§‹...':^78} {side_border}")
    print(bottom_border)
    print("\n")

def main():
    args = parse_args()
    set_seed(args.seed)
    
    # é…ç½®å¤šGPUè®­ç»ƒ
    args = setup_parallel_training(args)
    
    # åˆå§‹åŒ–wandb
    using_wandb = setup_wandb(args)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¸€æ¬¡è®­ç»ƒçš„æ£€æŸ¥ç‚¹
    last_checkpoint = None
    if os.path.isdir(args.output_dir):
        last_checkpoint = get_last_checkpoint(args.output_dir)
        if last_checkpoint is not None:
            logger.info(f"æ‰¾åˆ°æ£€æŸ¥ç‚¹: {last_checkpoint}ï¼Œå°†ä»æ­¤å¤„æ¢å¤è®­ç»ƒ")
    
    # åŠ è½½Qwenæ¨¡å‹å’Œåˆ†è¯å™¨
    logger.info(f"åŠ è½½æ¨¡å‹: {args.model_name_or_path}")
    
    model_config = AutoConfig.from_pretrained(args.model_name_or_path)
    
    # å¤„ç†æ»‘åŠ¨çª—å£æ³¨æ„åŠ›çš„è­¦å‘Š
    if hasattr(model_config, "sliding_window") and model_config.sliding_window is not None:
        logger.warning(f"æ¨¡å‹ä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›(window size={model_config.sliding_window})ï¼Œç¦ç”¨SDPAä»¥é¿å…è­¦å‘Š")
        os.environ["TRANSFORMERS_NO_SDPA"] = "true"
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœGPUå†…å­˜
    if args.gradient_checkpointing:
        model_config.use_cache = False
        logger.info("å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœGPUå†…å­˜")
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path, 
        config=model_config
    )
    
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
    
    # è®¾ç½®tokenizerå’Œæ¨¡å‹ç”¨äºé¢„è®­ç»ƒ
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ–‡ä»¶æ¨¡å¼: {args.file_pattern}")
    train_dataset = NovelChunksDataset(
        data_dir=args.data_dir,
        tokenizer=tokenizer,
        max_seq_length=args.max_seq_length,
        file_pattern=args.file_pattern
    )
    
    # è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    gpu_count = torch.cuda.device_count()
    effective_batch_size = args.per_device_train_batch_size * gpu_count * args.gradient_accumulation_steps
    
    # æ¼‚äº®åœ°æ‰“å°è®­ç»ƒé…ç½®
    print_training_config(args, model_config, train_dataset, effective_batch_size)
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # ä½¿ç”¨è‡ªå›å½’è¯­è¨€å»ºæ¨¡è€Œéæ©ç è¯­è¨€å»ºæ¨¡
    )
    
    # é…ç½®è®­ç»ƒå‚æ•°
    # ç¡®ä¿max_stepså’Œnum_train_epochsè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆå€¼
    if args.max_steps is None or args.max_steps <= 0:
        if args.num_train_epochs is None or args.num_train_epochs <= 0:
            logger.info("æ—¢æ²¡æœ‰è®¾ç½®æœ‰æ•ˆçš„max_stepsä¹Ÿæ²¡æœ‰è®¾ç½®æœ‰æ•ˆçš„num_train_epochsï¼Œè®¾ç½®é»˜è®¤å€¼max_steps=1000")
            max_steps = 1000
            num_train_epochs = None
        else:
            logger.info(f"ä½¿ç”¨num_train_epochs={args.num_train_epochs}è®­ç»ƒ")
            max_steps = -1
            num_train_epochs = args.num_train_epochs
    else:
        logger.info(f"ä½¿ç”¨max_steps={args.max_steps}è®­ç»ƒ")
        max_steps = args.max_steps
        num_train_epochs = None
    
    # æ ¹æ®ä¾èµ–æ£€æŸ¥ç»“æœè°ƒæ•´report_toè®¾ç½®
    report_options = []
    if args.use_wandb:
        report_options.append("wandb")
    if not tensorboard_missing:
        report_options.append("tensorboard")
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True if last_checkpoint is None else False,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        max_steps=max_steps,
        num_train_epochs=num_train_epochs,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        fp16=args.fp16,
        deepspeed=args.deepspeed,
        save_total_limit=3,  # ä»…ä¿å­˜æœ€å3ä¸ªæ£€æŸ¥ç‚¹
        remove_unused_columns=False,
        logging_dir=os.path.join(args.output_dir, "logs"),
        dataloader_num_workers=4,
        report_to=["wandb"] if args.use_wandb else [],
        # ç¡®ä¿å…¶ä»–å‚æ•°ä¹Ÿæœ‰é»˜è®¤å€¼
        local_rank=getattr(args, "local_rank", -1),
        ddp_find_unused_parameters=False,
    )
    
    # åˆå§‹åŒ–Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )
    
    # å¦‚æœå¯ç”¨wandbå¹¶è®¾ç½®äº†watchï¼Œç›‘æ§æ¨¡å‹
    if args.use_wandb and args.wandb_watch != "False":
        wandb.watch(
            model,
            log=args.wandb_watch,
            log_freq=args.logging_steps
        )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ")
    trainer.train(resume_from_checkpoint=last_checkpoint)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    # å¦‚æœå¯ç”¨äº†wandbæ¨¡å‹è®°å½•
    if args.use_wandb:
        logger.info("å°†æ¨¡å‹ä¸Šä¼ åˆ°Weights & Biases")
        artifact = wandb.Artifact(name=f"model-{wandb.run.id}", type="model")
        artifact.add_dir(args.output_dir)
        wandb.log_artifact(artifact)
    
    # å¦‚æœä½¿ç”¨wandbï¼Œå®Œæˆè¿è¡Œ
    if args.use_wandb:
        wandb.finish()
    
    logger.info("é¢„è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()
